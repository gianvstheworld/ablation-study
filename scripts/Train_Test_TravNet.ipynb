{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Importando bibliotecas úteis para a rede\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as torch_utils\n",
    "import torch.nn as nn\n",
    "\n",
    "# Importando TravNet e seu dataset\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "# from models.TravNet_WOBlocks import TravNet\n",
    "from models.TravNet_ViT import TravNet\n",
    "from utils.TravDataloader import TravNetDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "params = Object() # Cria um objeto para armazenar os parâmetros\n",
    "# Parametros do dataset\n",
    "params.data_path        = r'../../data/' \n",
    "params.csv_path         = os.path.join(params.data_path, 'data.csv')\n",
    "params.preproc          = True  # Vertical flip augmentation - inverte a imagem verticalmente\n",
    "params.depth_mean       = 3.5235\n",
    "params.depth_std        = 10.6645\n",
    "\n",
    "# Parametros de treino\n",
    "params.seed             = 230 # Seed para o gerador de números aleatórios - como saber a melhor seed para o modelo?\n",
    "params.epochs           = 25 # MUDAR AQUI AS ÉPOCAS\n",
    "params.batch_size       = 4\n",
    "params.learning_rate    = 1e-4\n",
    "params.weight_decay     = 1e-5\n",
    "\n",
    "# Parametros do modelo \n",
    "params.pretrained = True\n",
    "params.load_network_path = None \n",
    "params.input_size       = (424, 240)\n",
    "params.output_size      = (424, 240)\n",
    "params.output_channels  = 1\n",
    "params.bottleneck_dim   = 256\n",
    "\n",
    "# Parametros da ViT\n",
    "transformer_params = Object()\n",
    "transformer_params.in_channels = 3\n",
    "transformer_params.hidden_dim = 32\n",
    "transformer_params.num_heads = 4\n",
    "transformer_params.num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(params.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(params.seed) \n",
    "\n",
    "# Selecionar GPU ou CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Transformações para o dataset\n",
    "    # Pré-estudo ablativo - Alterar aqui\n",
    "    transform = transforms.Compose([\n",
    "                transforms.ToPILImage(), # Converte o tensor para uma imagem PIL\n",
    "                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "                transforms.RandomChoice([\n",
    "                    transforms.RandomHorizontalFlip(p=1),\n",
    "                    transforms.RandomRotation(15),\n",
    "                    transforms.RandomInvert(0.5),\n",
    "                ]),\n",
    "                transforms.ToTensor(), # Converte a imagem PIL para um tensor\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normaliza o tensor (média e desvio padrão)\n",
    "                ])\n",
    "    \n",
    "    '''\n",
    "    transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "    '''\n",
    "\n",
    "    dataset = TravNetDataset(params, transform)\n",
    "\n",
    "    # Divide o dataset em treino e validação \n",
    "    train_size, val_size = int(0.8*len(dataset)), np.ceil(0.2*len(dataset)).astype('int')\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size]) # Separa os dados de acordo com os tamanhos estabelecidos e embaralha\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params.batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=params.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    print('Total loaded %d images' % len(dataset))\n",
    "    print('Loaded %d train images' % train_size)\n",
    "    print('Loaded %d valid images' % val_size)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(binary_image):\n",
    "    binary_image_np = binary_image.detach().cpu().numpy()\n",
    "    binary_image_np = np.where(binary_image_np > 0.5, 255, 0).astype(np.uint8)\n",
    "\n",
    "    contours, _ = cv2.findContours(binary_image_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    bounding_boxes = []\n",
    "\n",
    "    for contour in contours:\n",
    "        x, y, largura, altura = cv2.boundingRect(contour)\n",
    "        bounding_boxes.append((x, y, largura, altura))\n",
    "\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(ground_truth, predicted):\n",
    "    iou_list = []\n",
    "\n",
    "    for gt_box, pred_box in zip(ground_truth, predicted):\n",
    "        x1_1, y1_1, largura_1, altura_1 = gt_box\n",
    "        x1_2, y1_2, largura_2, altura_2 = pred_box\n",
    "\n",
    "        x2_1 = x1_1 + largura_1\n",
    "        y2_1 = y1_1 + altura_1\n",
    "        x2_2 = x1_2 + largura_2\n",
    "        y2_2 = y1_2 + altura_2\n",
    "\n",
    "        x_overlap = max(0, min(x2_1, x2_2) - max(x1_1, x1_2))\n",
    "        y_overlap = max(0, min(y2_1, y2_2) - max(y1_1, y1_2))\n",
    "        intersection = x_overlap * y_overlap\n",
    "\n",
    "        gt_area = largura_1 * altura_1\n",
    "        pred_area = largura_2 * altura_2\n",
    "\n",
    "        if gt_area == 0 or pred_area == 0:\n",
    "            iou = 0.0  # Evitar divisão por zero\n",
    "        else:\n",
    "            iou = intersection / (gt_area + pred_area - intersection)\n",
    "        \n",
    "        iou_list.append(iou)\n",
    "\n",
    "    return iou_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes(path_img, pred_img):\n",
    "    bounding_box_path = get_bounding_box(path_img[0, 0, :, :])\n",
    "    bounding_box_pred = get_bounding_box(pred_img[0, 0, :, :])\n",
    "\n",
    "    iou_val = get_iou(bounding_box_path, bounding_box_pred)\n",
    "\n",
    "    path_img_np = path_img[0, 0, :, :].cpu().numpy()\n",
    "    pred_img_np = pred_img[0, 0, :, :].cpu().numpy()\n",
    "\n",
    "    combined_image = np.zeros_like(path_img_np)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(path_img_np, cmap='gray')  \n",
    "    plt.title(\"path_img com Bounding Boxes\")\n",
    "    for box in bounding_box_path:\n",
    "        x, y, w, h = box\n",
    "        plt.gca().add_patch(plt.Rectangle((x, y), w, h, fill=False, edgecolor='red'))\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(pred_img_np, cmap='gray')  \n",
    "    plt.title(\"pred_img com Bounding Boxes\")\n",
    "    for box in bounding_box_pred:\n",
    "        x, y, w, h = box\n",
    "        plt.gca().add_patch(plt.Rectangle((x, y), w, h, fill=False, edgecolor='blue'))\n",
    "    plt.show()\n",
    "\n",
    "    for box in bounding_box_pred:\n",
    "        x, y, w, h = box\n",
    "        combined_image[y:y+h, x:x+w] = 0.5 * pred_img_np[y:y+h, x:x+w] + 0.5 * combined_image[y:y+h, x:x+w]\n",
    "\n",
    "    for box in bounding_box_path:\n",
    "        x, y, w, h = box\n",
    "        combined_image[y:y+h, x:x+w] = 0.5 * path_img_np[y:y+h, x:x+w] + 0.5 * combined_image[y:y+h, x:x+w]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(combined_image, cmap='gray')\n",
    "    \n",
    "    if iou_val:  \n",
    "        plt.title(f\"IoU = {iou_val[0]:.2f}\") \n",
    "    else:\n",
    "        plt.title(\"IoU não disponível\")    \n",
    "    \n",
    "    for box in bounding_box_path:\n",
    "        x, y, w, h = box\n",
    "        plt.gca().add_patch(plt.Rectangle((x, y), w, h, fill=False, edgecolor='red'))\n",
    "    \n",
    "    for box in bounding_box_pred:\n",
    "        x, y, w, h = box\n",
    "        plt.gca().add_patch(plt.Rectangle((x, y), w, h, fill=False, edgecolor='blue'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-estudo ablativo - Alterar aqui\n",
    "def fit(net, criterion, optimizer, scheduler, train_loader, val_loader):    \n",
    "\n",
    "    # Pré-estudo ablativo - Alterar aqui\n",
    "    patience = 10 # Número de épocas sem melhora na perda de validação para parar o treinamento\n",
    "    counter = 0 \n",
    "\n",
    "    best_val_loss = np.inf \n",
    "    train_loss_list = [] \n",
    "    val_loss_list = [] \n",
    "\n",
    "    for epoch in range(params.epochs):\n",
    "        net.train()\n",
    "        train_loss = 0.0 \n",
    "        \n",
    "        for i, data in enumerate(train_loader):\n",
    "            data = (item.to(device).type(torch.float32) for item in data) \n",
    "            color_img, depth_img, path_img, mu_img, nu_img, weight = data \n",
    "\n",
    "            # Forward pass\n",
    "            pred = net(color_img, depth_img) # EDITAR AQUI CASO QUEIRA TESTAR COM/SEM PROFUNDIDADE\n",
    "            label = mu_img\n",
    "\n",
    "            loss = weight*criterion(pred*path_img, label) \n",
    "            loss = torch.mean(loss)\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "            train_loss += loss.item() \n",
    "\n",
    "        train_loss /= len(train_loader) \n",
    "        train_loss_list.append(train_loss) \n",
    "\n",
    "        if (epoch) % 10 == 0:\n",
    "            outstring = 'Epoch [%d/%d], Loss: ' % (epoch+1, params.epochs)\n",
    "            print(outstring, train_loss)\n",
    "            print('Learning Rate for this epoch: {}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "        # Testando o modelo \n",
    "        with torch.no_grad():\n",
    "            net.eval() \n",
    "\n",
    "            val_loss = 0.0 \n",
    "\n",
    "            for i, data in enumerate(val_loader):\n",
    "                data = (item.to(device).type(torch.float32) for item in data)\n",
    "                color_img, depth_img, path_img, mu_img, nu_img, weight = data\n",
    "\n",
    "                pred = net(color_img, depth_img) # EDITAR AQUI CASO QUEIRA TESTAR COM/SEM PROFUNDIDADE\n",
    "                label = mu_img\n",
    "\n",
    "                loss = weight*criterion(pred*path_img, label)\n",
    "                loss = torch.mean(loss)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_loss_list.append(val_loss)\n",
    "        \n",
    "            # Pré-estudo ablativo - Alterar aqui   \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "\n",
    "                # Salva o modelo\n",
    "                print('Updating best validation loss: %.5f' % best_val_loss)\n",
    "                torch.save(net.state_dict(), 'checkpoints/best_predictor_depth.pth')\n",
    "\n",
    "            else:\n",
    "                counter += 1\n",
    "                print('No improvement since last epoch: %d' % counter)\n",
    "                if counter >= patience:\n",
    "                    net.module.load_state_dict(torch.load('checkpoints/best_predictor_depth.pth'))\n",
    "                    print('Early stopping!')\n",
    "                    break\n",
    "\n",
    "        # Pré-estudo ablativo - Alterar aqui    \n",
    "        scheduler.step() \n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            plt.figure(figsize = (14,14))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(color_img[0].permute(1, 2, 0).cpu().numpy())\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(255*pred[0,0,:,:].detach().cpu().numpy(), vmin=0, vmax=255)\n",
    "            plt.show(block=False)\n",
    "            \n",
    "            draw_bounding_boxes(path_img, pred)\n",
    "\n",
    "    print('Training Loss list: ', train_loss_list)\n",
    "    print('Validation Loss list: ', val_loss_list)\n",
    "\n",
    "    plt.plot(train_loss_list, label='Treinamento')\n",
    "    plt.plot(val_loss_list, label='Validação')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Perda')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# REINICIALIZAR AQUI\n",
    "net = TravNet(params, transformer_params) # Instancia a rede - # EDITAR AQUI CASO QUEIRA TESTAR COM/SEM PROFUNDIDADE\n",
    "\n",
    "# Usado para carregar um modelo salvo\n",
    "if params.load_network_path is not None:\n",
    "    print('Loading saved network from {}'.format(params.load_network_path))\n",
    "    net.load_state_dict(torch.load(params.load_network_path))\n",
    "\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") \n",
    "net = torch.nn.DataParallel(net).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 19.29 GiB (GPU 0; 3.95 GiB total capacity; 118.73 MiB already allocated; 2.73 GiB free; 128.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/gian/Documentos/Códigos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb Célula 11\u001b[0m line \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Inicializa um tensor de teste\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# EDITAR AQUI CASO QUEIRA TESTAR COM/SEM PROFUNDIDADE\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test \u001b[39m=\u001b[39m net(torch\u001b[39m.\u001b[39;49mrand([\u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m, params\u001b[39m.\u001b[39;49minput_size[\u001b[39m1\u001b[39;49m], params\u001b[39m.\u001b[39;49minput_size[\u001b[39m0\u001b[39;49m]])\u001b[39m.\u001b[39;49mto(device), torch\u001b[39m.\u001b[39;49mrand([\u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m, params\u001b[39m.\u001b[39;49minput_size[\u001b[39m1\u001b[39;49m], params\u001b[39m.\u001b[39;49minput_size[\u001b[39m0\u001b[39;49m]])\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtest.shape:\u001b[39m\u001b[39m'\u001b[39m, test\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:166\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     kwargs \u001b[39m=\u001b[39m ({},)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49minputs[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[1;32m    168\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documentos/Códigos/IC/ablation-study/scripts/../src/models/TravNet_ViT.py:204\u001b[0m, in \u001b[0;36mTravNet.forward\u001b[0;34m(self, rgb_img, depth_img)\u001b[0m\n\u001b[1;32m    200\u001b[0m rgb_img \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(\n\u001b[1;32m    201\u001b[0m     rgb_img, size\u001b[39m=\u001b[39mdepth_img\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:], mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    203\u001b[0m \u001b[39m# Processar a entrada RGB e de profundidade através do Transformer Visual\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m visual_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual_transformer(rgb_img)\n\u001b[1;32m    206\u001b[0m \u001b[39m# Encoder - capturando contexto da imagem\u001b[39;00m\n\u001b[1;32m    207\u001b[0m out1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock1(rgb_img)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documentos/Códigos/IC/ablation-study/scripts/../src/models/TravNet_ViT.py:47\u001b[0m, in \u001b[0;36mVisualTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m B, C, H, W \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[1;32m     45\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(B, C, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# (N, B, C) onde N = H * W\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m     49\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mview(B, C, H, W)\n\u001b[1;32m     51\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documentos/Códigos/IC/ablation-study/scripts/../src/models/TravNet_ViT.py:23\u001b[0m, in \u001b[0;36mCustomTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x, x)\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py:141\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39mor\u001b[39;00m tgt\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model:\n\u001b[1;32m    139\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, mask\u001b[39m=\u001b[39;49msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[1;32m    142\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39mtgt_mask, memory_mask\u001b[39m=\u001b[39mmemory_mask,\n\u001b[1;32m    143\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    144\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39mmemory_key_padding_mask)\n\u001b[1;32m    145\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py:198\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    195\u001b[0m output \u001b[39m=\u001b[39m src\n\u001b[1;32m    197\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 198\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py:339\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    337\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    338\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    340\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    342\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py:347\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    346\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 347\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    348\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    349\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    350\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    351\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py:1003\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    992\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m    993\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m    994\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1001\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight)\n\u001b[1;32m   1002\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1004\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1005\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1006\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1007\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1008\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1009\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1010\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask)\n\u001b[1;32m   1011\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first:\n\u001b[1;32m   1012\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:5101\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   5096\u001b[0m     dropout_p \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m   5098\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   5099\u001b[0m \u001b[39m# (deep breath) calculate attention and out projection\u001b[39;00m\n\u001b[1;32m   5100\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m-> 5101\u001b[0m attn_output, attn_output_weights \u001b[39m=\u001b[39m _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)\n\u001b[1;32m   5102\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(tgt_len, bsz, embed_dim)\n\u001b[1;32m   5103\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:4844\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention\u001b[0;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[1;32m   4842\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(E)\n\u001b[1;32m   4843\u001b[0m \u001b[39m# (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\u001b[39;00m\n\u001b[0;32m-> 4844\u001b[0m attn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(q, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m   4845\u001b[0m \u001b[39mif\u001b[39;00m attn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4846\u001b[0m     attn \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m attn_mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 19.29 GiB (GPU 0; 3.95 GiB total capacity; 118.73 MiB already allocated; 2.73 GiB free; 128.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Inicializa um tensor de teste\n",
    "# EDITAR AQUI CASO QUEIRA TESTAR COM/SEM PROFUNDIDADE\n",
    "test = net(torch.rand([2, 3, params.input_size[1], params.input_size[0]]).to(device), torch.rand([2, 1, params.input_size[1], params.input_size[0]]).to(device))\n",
    "print('test.shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gian/Documentos/Códigos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb Célula 12\u001b[0m line \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_data, val_data \u001b[39m=\u001b[39m load_data()\n",
      "\u001b[1;32m/home/gian/Documentos/Códigos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb Célula 12\u001b[0m line \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m             transforms\u001b[39m.\u001b[39mToPILImage(), \u001b[39m# Converte o tensor para uma imagem PIL\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             transforms\u001b[39m.\u001b[39mColorJitter(brightness\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, contrast\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, saturation\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, hue\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m             transforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m]), \u001b[39m# Normaliza o tensor (média e desvio padrão)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m             ])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mtransform = transforms.Compose([\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m        transforms.ToPILImage(),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m        ])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m dataset \u001b[39m=\u001b[39m TravNetDataset(params, transform)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Divide o dataset em treino e validação \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gian/Documentos/C%C3%B3digos/IC/ablation-study/scripts/Train_Test_TravNet.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m train_size, val_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m0.8\u001b[39m\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(dataset)), np\u001b[39m.\u001b[39mceil(\u001b[39m0.2\u001b[39m\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(dataset))\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documentos/Códigos/IC/ablation-study/scripts/../src/utils/TravDataloader.py:40\u001b[0m, in \u001b[0;36mTravNetDataset.__init__\u001b[0;34m(self, params, transform)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolor_fname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth_fname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath_fname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmu_fname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnu_fname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n\u001b[1;32m     39\u001b[0m \u001b[39m# Prepara os pesos e o número de intervalos definidos \u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbins \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_weights()\n\u001b[1;32m     42\u001b[0m \u001b[39m# Obtém as estatísticas de profundidade\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_depth_stats()\n",
      "File \u001b[0;32m~/Documentos/Códigos/IC/ablation-study/scripts/../src/utils/TravDataloader.py:172\u001b[0m, in \u001b[0;36mTravNetDataset.prepare_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m path_fname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath_fname[idx]\n\u001b[1;32m    171\u001b[0m \u001b[39m# Carrega as imagens\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m mu_img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mimread(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot, mu_fname), \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    173\u001b[0m mu_img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(mu_img, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size, interpolation\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mINTER_AREA)\n\u001b[1;32m    174\u001b[0m mu_img \u001b[39m=\u001b[39m mu_img\u001b[39m/\u001b[39m\u001b[39m255.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data, val_data = load_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = iter(train_data)\n",
    "data = next(data_iterator)\n",
    "first_image = data[0][0]\n",
    "first_image = torch_utils.make_grid(first_image)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(first_image.permute(1, 2, 0))\n",
    "plt.title('First image')\n",
    "plt.show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training tools and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.L1Loss(reduction='none') # Perda L1 (erro absoluto médio)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=params.learning_rate, weight_decay=params.weight_decay) # Verificar o melhor otimizador para o modelo\n",
    "# Pré-estudo ablativo - Alterar aqui   \n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train = fit(net=net, criterion=criterion, optimizer=optimizer, train_loader=train_data, val_loader=val_data)\n",
    "# Pré-estudo ablativo - Alterar aqui \n",
    "train = fit(net=net, criterion=criterion, optimizer=optimizer, scheduler=scheduler, train_loader=train_data, val_loader=val_data) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
